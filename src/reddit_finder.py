# –§–∞–π–ª: src/reddit_finder.py (–í–ò–ü–†–ê–í–õ–ï–ù–û USER-AGENT)
import requests
from bs4 import BeautifulSoup
import textwrap
import random


def get_reddit_freebies(limit=5):
    """
    –ü–∞—Ä—Å–∏—Ç—å RSS-—Å—Ç—Ä—ñ—á–∫–∏ Reddit, –º–∞—Å–∫—É—é—á–∏—Å—å –ø—ñ–¥ –∑–≤–∏—á–∞–π–Ω–∏–π –±—Ä–∞—É–∑–µ—Ä.
    """
    rss_url = "https://www.reddit.com/r/freebies+giveaways+GameDeals+steam_giveaway/new/.rss"

    # –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û "–°–ü–†–ê–í–ñ–ù–Ü–ô" USER-AGENT (—è–∫ —É Chrome –Ω–∞ Windows)
    # –¶–µ –æ–±—Ö–æ–¥–∏—Ç—å –∑–∞—Ö–∏—Å—Ç Reddit, —è–∫–∏–π –≤—ñ–¥—Ö–∏–ª—è—î –∑–∞–ø–∏—Ç–∏ –≤—ñ–¥ —Å–∫—Ä–∏–ø—Ç—ñ–≤
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # timeout=10 —Å–µ–∫—É–Ω–¥, —â–æ–± –Ω–µ –≤–∏—Å—ñ–ª–æ –≤—ñ—á–Ω–æ
        response = requests.get(rss_url, headers=headers, timeout=5)

        if response.status_code == 429:
            return ["‚ö†Ô∏è –ó–∞–±–∞–≥–∞—Ç–æ –∑–∞–ø–∏—Ç—ñ–≤ –¥–æ Reddit. –°–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ —Ö–≤–∏–ª–∏–Ω—É."]

        if response.status_code != 200:
            return [f"‚ö†Ô∏è Reddit –≤—ñ–¥–ø–æ–≤—ñ–≤ –∫–æ–¥–æ–º {response.status_code}."]

        # –ü–∞—Ä—Å–∏–º–æ XML
        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')

        if not entries:
            return ["üì≠ –ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ —Å—Ç—Ä—ñ—á–∫—É Reddit. –ú–æ–∂–ª–∏–≤–æ, –∑–º—ñ–Ω–∏–≤—Å—è —Ñ–æ—Ä–º–∞—Ç."]

        results = []

        for entry in entries:
            try:
                title = entry.title.text
                link = entry.link['href']

                # –°–ø—Ä–æ–±–∞ –¥—ñ—Å—Ç–∞—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é
                category = "Reddit"
                if entry.category and 'term' in entry.category.attrs:
                    category = entry.category['term']
                elif entry.author and entry.author.find('name'):
                    category = entry.author.find('name').text.replace('/u/', '')

                # –û–ø–∏—Å
                content = entry.content.text if entry.content else ""
                clean_desc = BeautifulSoup(content, "html.parser").get_text()
                short_desc = textwrap.shorten(clean_desc, width=150, placeholder="...")

                results.append({
                    "source": f"r/{category}",
                    "title": title,
                    "link": link,
                    "description": short_desc
                })

                if len(results) >= limit:
                    break
            except Exception as parse_error:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ "–±–∏—Ç–∏–π" –ø–æ—Å—Ç, –π–¥–µ–º–æ –¥–∞–ª—ñ

        if not results:
            return "üì≠ –ù–∞ Reddit –∑–∞—Ä–∞–∑ —Ç–∏—Ö–æ. –ù–æ–≤–∏—Ö —Ä–æ–∑–¥–∞—á –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ."

        return results

    except requests.exceptions.ConnectTimeout:
        return ["‚ö†Ô∏è –ß–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è Reddit –≤–∏—á–µ—Ä–ø–∞–≤—Å—è (Timeout). –°–ø—Ä–æ–±—É–π—Ç–µ –ø—ñ–∑–Ω—ñ—à–µ."]
    except requests.exceptions.ConnectionError:
        return ["‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ Reddit. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç."]
    except Exception as e:
        print(f"Global Reddit Error: {e}")
        return [f"‚ùå –ü–æ–º–∏–ª–∫–∞: {str(e)[:50]}"]


# –¢–µ—Å—Ç
if __name__ == "__main__":
    print("üîç –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è Reddit RSS...")
    items = get_reddit_freebies(5)
    if isinstance(items, list):
        for item in items:
            if isinstance(item, dict):
                print(f"- [{item['source']}] {item['title']}")
            else:
                print(item)
    else:
        print(items)